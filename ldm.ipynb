{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":39911,"sourceType":"datasetVersion","datasetId":31296}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from transformers import CLIPProcessor, CLIPModel, CLIPTokenizer\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchvision.transforms as transforms\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.amp import autocast, GradScaler\n\nfrom tqdm import tqdm\nimport numpy as np\nimport pandas as pd\nimport os\nimport random\nimport math\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nimport os","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-06-22T23:03:48.573706Z","iopub.execute_input":"2025-06-22T23:03:48.574384Z","iopub.status.idle":"2025-06-22T23:03:48.578701Z","shell.execute_reply.started":"2025-06-22T23:03:48.574360Z","shell.execute_reply":"2025-06-22T23:03:48.578174Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"device = 'cuda' if torch.cuda.is_available() else 'cpu'\nos.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-22T23:03:50.672580Z","iopub.execute_input":"2025-06-22T23:03:50.673153Z","iopub.status.idle":"2025-06-22T23:03:50.676499Z","shell.execute_reply.started":"2025-06-22T23:03:50.673129Z","shell.execute_reply":"2025-06-22T23:03:50.675800Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"clip_model = CLIPModel.from_pretrained('openai/clip-vit-base-patch32').to(device)\nclip_processor = CLIPProcessor.from_pretrained('openai/clip-vit-base-patch32')\nclip_tokenizer = CLIPTokenizer.from_pretrained('openai/clip-vit-base-patch32')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-22T23:04:27.052930Z","iopub.execute_input":"2025-06-22T23:04:27.053205Z","iopub.status.idle":"2025-06-22T23:04:28.865407Z","shell.execute_reply.started":"2025-06-22T23:04:27.053184Z","shell.execute_reply":"2025-06-22T23:04:28.864691Z"},"scrolled":true},"outputs":[],"execution_count":5},{"cell_type":"code","source":"images_path = '/kaggle/input/flickr-image-dataset/flickr30k_images/flickr30k_images/'\ncsv_path = '/kaggle/input/flickr-image-dataset/flickr30k_images/results.csv'\ncaptions = pd.read_csv(csv_path, sep='|')\ncaptions.rename(columns={' comment': 'comment'}, inplace=True)\ndataset = [(captions.iloc[i][\"comment\"], os.path.join(images_path, captions.iloc[i][\"image_name\"])) for i in range(len(captions))]\n\ndataset_sz = len(dataset)\ntrain_sz = int(dataset_sz * 0.6)\nval_sz = int(dataset_sz * 0.2)\n\ntrain_set = dataset[:train_sz]\nval_set = dataset[train_sz:train_sz + val_sz]\ntest_set = dataset[train_sz + val_sz:]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-22T23:04:31.639892Z","iopub.execute_input":"2025-06-22T23:04:31.640139Z","iopub.status.idle":"2025-06-22T23:04:37.402884Z","shell.execute_reply.started":"2025-06-22T23:04:31.640121Z","shell.execute_reply":"2025-06-22T23:04:37.402059Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"class LDMDataset(Dataset):\n    def __init__(self, img_texts, transform=None):\n        super().__init__()\n        self.img_texts = img_texts\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.img_texts)\n\n    def __getitem__(self, index):\n        caption, img_path = self.img_texts[index]\n        img = Image.open(img_path).convert('RGB')\n        if self.transform:\n            img = self.transform(img)\n        return img, caption","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-22T23:04:37.896389Z","iopub.execute_input":"2025-06-22T23:04:37.896906Z","iopub.status.idle":"2025-06-22T23:04:37.904241Z","shell.execute_reply.started":"2025-06-22T23:04:37.896879Z","shell.execute_reply":"2025-06-22T23:04:37.903414Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"transform = transforms.Compose([\n    transforms.Resize((512, 512)),\n    transforms.ToTensor(),\n    transforms.Normalize([0.5]*3, [0.5]*3)\n])\n\ntrain_dataset = LDMDataset(train_set, transform)\nval_dataset = LDMDataset(val_set, transform)\ntest_dataset = LDMDataset(test_set, transform)\n\ntrain_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=3, pin_memory=True)\nval_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=3, pin_memory=True)\ntest_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=3, pin_memory=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-22T23:04:38.105669Z","iopub.execute_input":"2025-06-22T23:04:38.106020Z","iopub.status.idle":"2025-06-22T23:04:38.111246Z","shell.execute_reply.started":"2025-06-22T23:04:38.106000Z","shell.execute_reply":"2025-06-22T23:04:38.110569Z"}},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":"## Model Architecture","metadata":{}},{"cell_type":"markdown","source":"### VAE","metadata":{}},{"cell_type":"code","source":"class VAEResBlock(nn.Module):\n    def __init__(self, cin, cout):\n        super().__init__()\n        self.norm1 = nn.GroupNorm(32, cin)\n        self.act1 = nn.SiLU()\n        self.conv1 = nn.Conv2d(cin, cout, kernel_size=3, padding=1)\n\n        self.norm2 = nn.GroupNorm(32, cout)\n        self.act2 = nn.SiLU()\n        self.conv2 = nn.Conv2d(cout, cout, kernel_size=3, padding=1)\n\n        if cin != cout:\n            self.shortcut = nn.Conv2d(cin, cout, kernel_size=1)\n        else:\n            self.shortcut = nn.Identity()\n\n    def forward(self, x):\n        out = self.conv1(self.act1(self.norm1(x)))\n        out = self.conv2(self.act2(self.norm2(out)))\n        return out + self.shortcut(x)\n\n\nclass VAEEncoderBlock(nn.Module):\n    def __init__(self, cin, cout):\n        super().__init__()\n        self.conv = nn.Conv2d(cin, cout, kernel_size=3, padding=1)\n        self.res = VAEResBlock(cout, cout)\n        self.down = nn.Conv2d(cout, cout, kernel_size=3, stride=2, padding=1)\n\n    def forward(self, x):\n        return self.down(self.res(self.conv(x)))\n\n\nclass VAEEncoder(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.block1 = VAEEncoderBlock(3, 128)\n        self.block2 = VAEEncoderBlock(128, 256)\n        self.block3 = VAEEncoderBlock(256, 512)\n\n        self.norm = nn.GroupNorm(32, 512)\n        self.act = nn.SiLU()\n        self.conv = nn.Conv2d(512, 8, kernel_size=3, stride=1, padding=1)\n\n    def forward(self, x): # (B, 3, 512, 512)\n        out = self.block1(x) # (B, 128, 256, 256)\n        out = self.block2(out) # (B, 256, 128, 128)\n        out = self.block3(out) # (B, 512, 64, 64)\n        out = self.conv(self.act(self.norm(out))) # (B, 8, 64, 64)\n\n        mu, logvar = torch.chunk(out, chunks=2, dim=1)\n        std = torch.exp(0.5 * logvar)\n        noise = torch.randn_like(std)\n        z = mu + std * noise\n        return z, mu, logvar\n\n\nclass VAEDecoderBlock(nn.Module):\n    def __init__(self, cin, cout):\n        super().__init__()\n        self.conv = nn.Conv2d(cin, cout, kernel_size=3, padding=1)\n        self.res = VAEResBlock(cout, cout)\n        self.up = nn.ConvTranspose2d(cout, cout, kernel_size=4, stride=2, padding=1)\n\n    def forward(self, x):\n        return self.up(self.res(self.conv(x)))\n\n\nclass VAEDecoder(nn.Module):\n    def __init__(self):\n        super().__init__() # (B, 4, 64, 64)\n        self.conv = nn.Conv2d(4, 8, kernel_size=3, padding=1) # (B, 8, 64, 64)\n        \n        self.block1 = VAEDecoderBlock(8, 512) # (B, 512, 128, 128)\n        self.block2 = VAEDecoderBlock(512, 256) # (B, 256, 256, 256)\n        self.block3 = VAEDecoderBlock(256, 128) # (B, 128, 512, 512)\n\n        self.conv2 = nn.Conv2d(128, 3, kernel_size=3, padding=1)\n        self.act = nn.Tanh()\n\n    def forward(self, x):\n        out = self.conv(x)\n        out = self.block3(self.block2(self.block1(out)))\n        out = self.conv2(out)\n        return self.act(out)\n\n\nclass VAE(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.encoder = VAEEncoder()\n        self.decoder = VAEDecoder()\n\n    def forward(self, x):\n        z, mu, logvar = self.encoder(x)\n        recon = self.decoder(z)\n        return recon, mu, logvar","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-22T23:04:40.516599Z","iopub.execute_input":"2025-06-22T23:04:40.517392Z","iopub.status.idle":"2025-06-22T23:04:40.533390Z","shell.execute_reply.started":"2025-06-22T23:04:40.517367Z","shell.execute_reply":"2025-06-22T23:04:40.532615Z"}},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":"### DDPM","metadata":{}},{"cell_type":"code","source":"class SinusoidalTimeEmbeddings(nn.Module):\n    def __init__(self, cout=1280):\n        super().__init__()\n        self.cout = cout\n\n    def forward(self, timesteps):\n        half_dim = self.cout // 2\n        \n        timesteps = timesteps.float()\n        i = torch.arange(half_dim, dtype=torch.float32, device=timesteps.device)\n        exponent = -math.log(10000) * i / half_dim\n        freqs = torch.exp(exponent)\n        \n        angles = timesteps[:, None] * freqs[None, :]\n        emb = torch.zeros(timesteps.size(0), self.cout, device=timesteps.device)\n        emb[:, 0::2] = torch.sin(angles)\n        emb[:, 1::2] = torch.cos(angles)\n        \n        return emb\n\n\nclass CrossAttention(nn.Module):\n    def __init__(self, cin_img, dim, cin_txt=512, n_heads=8):\n        super().__init__()\n        self.cin_img = cin_img\n        self.cin_txt = cin_txt\n        \n        self.w_k = nn.Linear(cin_txt, dim)\n        self.w_v = nn.Linear(cin_txt, dim)\n        self.w_q = nn.Linear(cin_img, dim)\n\n        self.dim = dim\n        self.n_heads = n_heads\n        self.dim_head = dim // n_heads\n\n        if cin_img == dim:\n            self.shortcut = nn.Identity()\n        else:\n            self.shortcut = nn.Conv2d(cin_img, dim, kernel_size=1)\n        self.lin_proj = nn.Conv2d(dim, dim, kernel_size=1)\n\n    def forward(self, x, y, y_mask): # x = image, y = text\n        batch_sz, cin, h, w = x.shape # (B, C, H, W)\n        l = h*w\n        img_out = x.reshape(batch_sz, cin, l).transpose(1, 2) # (B, H*W, C)\n        \n        q = self.w_q(img_out) # (B, H*W, d_k)\n        k = self.w_k(y) # (B, n_max, d_k)\n        v = self.w_v(y) # (B, n_max, d_k)\n\n        q = q.reshape(batch_sz, l, self.n_heads, self.dim_head).transpose(1, 2) # (B, n_h, H*W, d_h)\n        k = k.reshape(batch_sz, k.size(1), self.n_heads, self.dim_head).transpose(1, 2) # (B, n_h, n_max, d_h)\n        v = v.reshape(batch_sz, v.size(1), self.n_heads, self.dim_head).transpose(1, 2) # (B, n_h, n_max, d_h)\n\n        scale = math.sqrt(self.dim_head)\n        att = torch.softmax((q @ k.transpose(-2, -1)) / scale, dim=-1)  # (B, n_h, H*W, n_max)\n        y_mask = y_mask[:, None, None, :] # (B, 1, 1, n_max)\n        att = att.masked_fill_(y_mask == 0, -1e9)\n        out = att @ v # (B, n_h, H*W, d_h)\n        out = out.transpose(1, 2).reshape(batch_sz, h, w, self.dim) # (B, H, W, D)\n        out = out.permute(0, 3, 1, 2) # (B, D, H, W)\n\n        return self.shortcut(x) + self.lin_proj(out) # (B, D, H, W)\n\n\nclass ResLayer(nn.Module):\n    def __init__(self, cin, cout, kernel_size=3, stride=1, padding=1,\n                 has_norm=True, has_act=True):\n        super().__init__()\n        self.norm = nn.GroupNorm(min(32, cin), cin) if has_norm else nn.Identity()\n        self.act = nn.SiLU() if has_act else nn.Identity()\n        self.conv = nn.Conv2d(cin, cout, kernel_size=kernel_size, stride=stride, padding=padding)\n\n    def forward(self, x):\n        return self.conv(self.act(self.norm(x)))\n\n\nclass ResBlock(nn.Module):\n    def __init__(self, cin, cmid, cout, time_emb_dim=1280, downsample=False):\n        super().__init__()\n        self.res_layer1 = ResLayer(cin, cmid)\n        self.mlp = nn.Linear(time_emb_dim, cmid)\n\n        stride = 2 if downsample else 1\n        \n        self.res_layer2 = ResLayer(cmid, cout, stride=stride)\n\n        if cin != cout or downsample: \n            self.shortcut = nn.Conv2d(cin, cout, kernel_size=1, stride=stride)\n        else:\n            self.shortcut = nn.Identity()\n\n    def forward(self, x, timesteps):\n        out = self.res_layer1(x) # (B, Cmid, H, W)\n        time_emb = self.mlp(timesteps)[:, :, None, None] # (B, Cmid, 1, 1)\n        out = out + time_emb\n        out = self.res_layer2(out)\n        return self.shortcut(x) + out\n\n\nclass DiffEncoderBlock(nn.Module):\n    def __init__(self, cin, cout, has_attn=False,\n                 kernel_size=3, stride=2, padding=1):\n        super().__init__()\n        self.res1 = ResBlock(cin, cin, cin)\n        self.res2 = ResBlock(cin, cin, cin)\n        self.attn = (lambda x, y, y_mask : x) if not has_attn else CrossAttention(cin_img=cin, dim=cin)\n        self.conv = nn.Conv2d(cin, cout, kernel_size=kernel_size,\n                              stride=stride, padding=padding)\n\n    def forward(self, x, y, y_mask, time_emb): # x = images, y = text, time_emb = timesteps\n        out = self.res1(x, time_emb)\n        out = self.res2(out, time_emb)\n        out = self.attn(out, y, y_mask)\n        return out, self.conv(out)\n\n\nclass DiffEncoder(nn.Module):\n    def __init__(self):\n        super().__init__() # (B, 4, 64, 64)\n        self.conv = nn.Conv2d(4, 320, kernel_size=3, padding=1) # (B, 320, 64, 64)\n        self.block4 = DiffEncoderBlock(320, 320, kernel_size=1, stride=1, padding=0) # (B, 320, 64, 64)\n        self.block3 = DiffEncoderBlock(320, 640) # (B, 640, 32, 32)\n        self.block2 = DiffEncoderBlock(640, 960, has_attn=True) # (B, 960, 16, 16)\n        self.block1 = DiffEncoderBlock(960, 1280, has_attn=True) # (B, 1280, 8, 8)\n        \n    def forward(self, x, y, y_mask, time_emb):\n        out = self.conv(x)\n        pre_pool4, out = self.block4(out, y, y_mask, time_emb)\n        pre_pool3, out = self.block3(out, y, y_mask, time_emb)\n        pre_pool2, out = self.block2(out, y, y_mask, time_emb)\n        pre_pool1, out = self.block1(out, y, y_mask, time_emb)\n        return pre_pool1, pre_pool2, pre_pool3, pre_pool4, out\n\n\nclass DiffBottleneck(nn.Module):\n    def __init__(self, cin=1280):\n        super().__init__()\n        self.res1 = ResBlock(cin, cin, cin)\n        self.res2 = ResBlock(cin, cin, cin)\n        self.attn = CrossAttention(cin_img=cin, dim=cin)\n\n    def forward(self, x, y, y_mask, time_emb):\n        return self.res2(self.attn(self.res1(x, time_emb), y, y_mask), time_emb)\n\n\nclass DiffDecoderBlock(nn.Module):\n    def __init__(self, cin, cout, has_attn=False,\n                 kernel_size=3, stride=1, padding=1):\n        super().__init__()\n        self.up = nn.ConvTranspose2d(cin, cin, kernel_size=kernel_size,\n                                     stride=stride, padding=padding)\n        self.res1 = ResBlock(cin + cout, cin + cout, cout)\n        self.res2 = ResBlock(cout, cout, cout)\n        self.attn = (lambda x, y, y_mask: x) if not has_attn else CrossAttention(cin_img=cout, dim=cout)\n\n    def forward(self, x, y, y_mask, time_emb, skip):\n        out = self.up(x)\n        out = torch.cat([out, skip], dim=1)\n        out = self.res1(out, time_emb)\n        out = self.attn(out, y, y_mask)\n        return self.res2(out, time_emb)\n\n\nclass DiffDecoder(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.block1 = DiffDecoderBlock(1280, 960, has_attn=True, kernel_size=4, stride=2)\n        self.block2 = DiffDecoderBlock(960, 640, has_attn=True, kernel_size=4, stride=2)\n        self.block3 = DiffDecoderBlock(640, 320, kernel_size=4, stride=2)\n        self.block4 = DiffDecoderBlock(320, 320)\n\n        self.norm = nn.GroupNorm(32, 320)\n        self.act = nn.SiLU()\n        self.conv = nn.Conv2d(320, 4, kernel_size=3, padding=1)\n\n    def forward(self, x, y, y_mask, time_emb, pre_pool1, pre_pool2, pre_pool3, pre_pool4):\n        out = self.block1(x, y, y_mask, time_emb, pre_pool1)\n        out = self.block2(out, y, y_mask, time_emb, pre_pool2)\n        out = self.block3(out, y, y_mask, time_emb, pre_pool3)\n        out = self.block4(out, y, y_mask, time_emb, pre_pool4)\n        return self.conv(self.act(self.norm(out)))\n\n\nclass LDMDiffusion(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.time_emb = SinusoidalTimeEmbeddings()\n        self.encoder = DiffEncoder()\n        self.bottleneck = DiffBottleneck()\n        self.decoder = DiffDecoder()\n        \n\n    def forward(self, x, y, y_mask, time):\n        time_emb = self.time_emb(time)\n        pre_pool1, pre_pool2, pre_pool3, pre_pool4, out = self.encoder(x, y, y_mask, time_emb)\n        out = self.bottleneck(out, y, y_mask, time_emb)\n        out = self.decoder(out, y, y_mask, time_emb,\n                           pre_pool1, pre_pool2, pre_pool3, pre_pool4)\n        return out","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-22T23:04:42.553511Z","iopub.execute_input":"2025-06-22T23:04:42.553837Z","iopub.status.idle":"2025-06-22T23:04:42.581178Z","shell.execute_reply.started":"2025-06-22T23:04:42.553813Z","shell.execute_reply":"2025-06-22T23:04:42.580468Z"}},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":"## Training","metadata":{}},{"cell_type":"markdown","source":"### VAE training","metadata":{}},{"cell_type":"code","source":"def vae_loss_function(recon, x, mu, logvar):\n    recon_loss = F.mse_loss(recon, x, reduction='mean')\n    kl_div = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp()) / x.size(0)\n    return recon_loss + kl_div","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-22T23:04:47.920197Z","iopub.execute_input":"2025-06-22T23:04:47.920458Z","iopub.status.idle":"2025-06-22T23:04:47.924377Z","shell.execute_reply.started":"2025-06-22T23:04:47.920440Z","shell.execute_reply":"2025-06-22T23:04:47.923794Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"vae = VAE().to(device)\ndummy = torch.randn(1, 3, 256, 256).to(device)\nrecon, mu, logvar = vae(dummy)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-22T23:04:48.534703Z","iopub.execute_input":"2025-06-22T23:04:48.535010Z","iopub.status.idle":"2025-06-22T23:04:49.389377Z","shell.execute_reply.started":"2025-06-22T23:04:48.534990Z","shell.execute_reply":"2025-06-22T23:04:49.388860Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"vae_model_path = '/kaggle/working/vae.pth'\n\nvae = VAE().to(device)\noptimizer = torch.optim.Adam(vae.parameters(), lr=2e-4)\nscaler = GradScaler()\n\nnum_epochs = 20\nbest_val_loss = float('inf')\npatience = 4\nnum_cons_bad_epochs = 0\n\nfor epoch in range(num_epochs):\n    torch.cuda.empty_cache()\n\n    train_loss = 0.0\n    train_total = 0\n\n    print(f\"\\nEpoch [{epoch+1}/{num_epochs}]\")\n\n    vae.train()\n    train_bar = tqdm(train_loader, desc=\"Training\", leave=False)\n    for imgs, _ in train_bar:\n        imgs = imgs.to(device)\n\n        optimizer.zero_grad()\n        with autocast(device_type=device):\n            recon, mu, logvar = vae(imgs)\n            loss = vae_loss_function(recon, imgs, mu, logvar)\n\n        scaler.scale(loss).backward()\n        scaler.step(optimizer)\n        scaler.update()\n\n        train_loss += loss.item() * imgs.size(0)\n        train_total += imgs.size(0)\n        train_bar.set_postfix(loss=loss.item())\n\n    vae.eval()\n    val_loss = 0.0\n    val_total = 0\n    val_bar = tqdm(val_loader, desc=\"Validation\", leave=False)\n    with torch.no_grad():\n        for imgs, _ in val_bar:\n            imgs = imgs.to(device)\n            with autocast(device_type=device):\n                recon, mu, logvar = vae(imgs)\n                loss = vae_loss_function(recon, imgs, mu, logvar)\n\n            val_loss += loss.item() * imgs.size(0)\n            val_total += imgs.size(0)\n            val_bar.set_postfix(loss=loss.item())\n\n    avg_train = train_loss / train_total\n    avg_val   = val_loss / val_total\n    print(f\"Train Loss: {avg_train:.4f} | Val Loss: {avg_val:.4f}\")\n\n    if avg_val < best_val_loss:\n        best_val_loss = avg_val\n        num_cons_bad_epochs = 0\n        torch.save(vae.state_dict(), vae_model_path)\n        print(\"Model saved.\")\n    else:\n        num_cons_bad_epochs += 1\n        if num_cons_bad_epochs == patience:\n            print(\"Early stopping.\")\n            break\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"vae = VAE().to(device)\nvae.load_state_dict(torch.load(vae_model_path))\nvae.eval()\n\ntest_loss = 0.0\ntest_total = 0\ntest_bar = tqdm(test_loader, desc=\"Test\", leave=False)\nwith torch.no_grad():\n    for imgs, _ in test_bar:\n        imgs = imgs.to(device)\n        with autocast(device_type=device):\n            recon, mu, logvar = vae(imgs)\n            loss = vae_loss_function(recon, imgs, mu, logvar)\n\n        test_loss += loss.item() * imgs.size(0)\n        test_total += imgs.size(0)\n        test_bar.set_postfix(loss=loss.item())\n\navg_test = test_loss / test_total\nprint(f\"Test Loss: {avg_test:.4f}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### DDPM training","metadata":{}},{"cell_type":"code","source":"def beta_linear_schedule(beta_start=1e-4, beta_end=0.02, steps=1000):\n    return torch.linspace(beta_start, beta_end, steps)\n    # tensor([0, 0.25, 0.5, 0.75, 1])\n\ndef get_alpha_cumprods(betas):\n    alphas = 1. - betas\n    alpha_bars = torch.cumprod(alphas, dim=0)\n    return alpha_bars\n\ndef noisify_image(x0, t, alpha_bars):\n    # x0 --> 1, c, h, w or h, w, c\n    sqrt_alpha_bar = alpha_bars[t].sqrt()\n    noise = torch.randn_like(x0)\n    noisy_img = sqrt_alpha_bar * x0 + (1 - sqrt_alpha_bar) * noise\n    return noisy_img\n\ndef noisify_images(x0, t, alpha_bars):\n    # x0 --> b, c, h, w\n    sqrt_alpha_bar = alpha_bars[t].sqrt().view(-1, 1, 1, 1) # (B, 1, 1, 1)\n    sqrt_one_minus = (1 - alpha_bars[t]).sqrt().view(-1, 1, 1, 1) # (B, 1, 1, 1)\n    noise = torch.randn_like(x0)\n    return sqrt_alpha_bar * x0 + sqrt_one_minus * noise","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"T = 1000\nbetas = beta_linear_schedule(0.0001, 0.02, T).to(device)\nalpha_bars = get_alpha_cumprods(betas).to(device)\n\nddpm_model_path = '/kaggle/working/ddpm.pth'\nvae_model_path = '/kaggle/working/vae.pth'\n\nvae = VAE().to(device)\nvae.load_state_dict(torch.load(vae_model_path))\nvae.eval()\n\nddpm = LDMDiffusion().to(device)\noptimizer = torch.optim.Adam(ddpm.parameters(), lr=2e-4)\nscaler = GradScaler()\n\nnum_epochs = 20\nbest_val_loss = float('inf')\npatience = 4\nnum_cons_bad_epochs = 0\n\nfor epoch in range(num_epochs):\n    torch.cuda.empty_cache()\n    \n    train_loss = 0.0\n    train_total = 0\n\n    ddpm.train()\n    for imgs, captions in train_loader:\n        batch_size = imgs.size(0)\n        imgs = imgs.to(device)\n    \n        with torch.no_grad():\n            z, _, _ = vae.encoder(imgs)\n        \n        captions = [\"\" if random.random() < 0.1 else cap for cap in captions]\n    \n        caption_inputs = clip_tokenizer(captions, padding=True, truncation=True,\n                                        return_tensors=\"pt\", return_attention_mask=True)\n        caption_inputs = {k: v.to(device) for k, v in caption_inputs.items()}\n        caption_outputs = clip_model.text_model(**caption_inputs, output_hidden_states=True)\n        text_emb = caption_outputs.last_hidden_state\n        text_att_mask = caption_inputs[\"attention_mask\"]\n    \n        timesteps = torch.randint(low=0, high=T, size=(batch_size,), device=device).long()\n        noise = torch.randn_like(z)\n        alpha_t = alpha_bars[timesteps].view(-1, 1, 1, 1)\n        sqrt_alpha = alpha_t.sqrt()\n        sqrt_one_minus = (1 - alpha_t).sqrt()\n        noisy_z = sqrt_alpha * z + sqrt_one_minus * noise\n\n        with autocast(device_type=device):\n            pred_noise = ddpm(noisy_z, text_emb, text_att_mask, timesteps)\n            loss = F.mse_loss(pred_noise, noise)\n            \n        optimizer.zero_grad()\n        scaler.scale(loss).backward()\n        scaler.step(optimizer)\n        scaler.update()\n    \n        train_loss += loss.item() * batch_size\n        train_total += batch_size\n\n    val_loss = 0.0\n    val_total = 0\n    \n    ddpm.eval()\n    for imgs, captions in val_loader:\n        with torch.no_grad():\n            batch_size = imgs.size(0)\n            imgs = imgs.to(device)\n\n            with torch.no_grad():\n                z, _, _ = vae.encoder(imgs)\n\n            captions = [\"\" if random.random() < 0.1 else caption for caption in captions]\n    \n            caption_inputs = clip_tokenizer(captions, padding=True, truncation=True,\n                                            return_tensors=\"pt\", return_attention_mask=True)\n            caption_inputs = {k: v.to(device) for k, v in caption_inputs.items()}\n            caption_outputs = clip_model.text_model(**caption_inputs, output_hidden_states=True)\n            text_emb = caption_outputs.last_hidden_state\n            text_att_mask = caption_inputs[\"attention_mask\"]\n        \n            timesteps = torch.randint(low=0, high=T, size=(batch_size,), device=device).long()\n            noise = torch.randn_like(z)\n            alpha_t = alpha_bars[timesteps].view(-1, 1, 1, 1)\n            sqrt_alpha = alpha_t.sqrt()\n            sqrt_one_minus = (1 - alpha_t).sqrt()\n            noisy_z = sqrt_alpha * z + sqrt_one_minus * noise\n\n            with autocast(device_type=device):\n                pred_noise = ddpm(noisy_z, text_emb, text_att_mask, timesteps)\n                loss = F.mse_loss(pred_noise, noise)\n\n            val_loss += loss.item() * batch_size\n            val_total += batch_size\n\n    avg_train = train_loss / train_total\n    avg_val   = val_loss / val_total\n    print(f\"Train Loss: {avg_train:.4f} | Val Loss: {avg_val:.4f}\")\n\n    if avg_val < best_val_loss:\n        best_val_loss = avg_val\n        num_cons_bad_epochs = 0\n        torch.save(ddpm.state_dict(), ddpm_model_path)\n        print(\"Model saved.\")\n    else:\n        num_cons_bad_epochs += 1\n        if num_cons_bad_epochs == patience:\n            print(\"Early stopping.\")\n            break","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"ddpm = LDMDiffusion().to(device)\nvae = VAE().to(device)\nddpm.load_state_dict(torch.load(ddpm_model_path))\nvae.load_state_dict(torch.load(vae_model_path))\nddpm.eval()\nvae.eval()\n\ntest_loss = 0.0\ntest_total = 0\n\nfor imgs, captions in test_loader:\n    with torch.no_grad():\n        batch_size = imgs.size(0)\n        imgs = imgs.to(device)\n\n        z, _, _ = vae.encoder(imgs)\n        captions = list(captions)\n\n        caption_inputs = clip_tokenizer(captions, padding=True, truncation=True,\n                                        return_tensors=\"pt\", return_attention_mask=True)\n        caption_inputs = {k: v.to(device) for k, v in caption_inputs.items()}\n        caption_outputs = clip_model.text_model(**caption_inputs, output_hidden_states=True)\n        text_emb = caption_outputs.last_hidden_state\n        text_att_mask = caption_inputs[\"attention_mask\"]\n\n        timesteps = torch.randint(low=0, high=T, size=(batch_size,), device=device).long()\n        noise = torch.randn_like(z)\n        alpha_t = alpha_bars[timesteps].view(-1, 1, 1, 1)\n        sqrt_alpha = alpha_t.sqrt()\n        sqrt_one_minus = (1 - alpha_t).sqrt()\n        noisy_z = sqrt_alpha * z + sqrt_one_minus * noise\n\n        with autocast(device_type=device):\n            pred_noise = ddpm(noisy_z, text_emb, text_att_mask, timesteps)\n            loss = F.mse_loss(pred_noise, noise)\n\n        test_loss += loss.item() * batch_size\n        test_total += batch_size\n\navg_test = test_loss / test_total\nprint(f\"Test Loss: {avg_test:.4f}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Generate","metadata":{}},{"cell_type":"markdown","source":"### Classifier-Free Guidance (CFG)","metadata":{}},{"cell_type":"code","source":"ddpm_model_path = '/kaggle/working/ddpm.pth'\nvae_model_path = '/kaggle/working/vae.pth'\n\nvae = VAE().to(device)\nddpm = LDMDiffusion().to(device)\nvae.load_state_dict(torch.load(vae_model_path))\nddpm.load_state_dict(torch.load(ddpm_model_path))\nvae.eval()\nddpm.eval()\n\ncfg_scale = 3.0\n\nbatch_size = 1\ncaption = [\"purple cat on a windowsill\"]\nlatent_shape = (batch_size, 4, 64, 64)\ncurr_state = torch.randn(latent_shape).to(device)\n\ncaption_inputs = clip_tokenizer(caption, padding=True, truncation=True, return_tensors=\"pt\")\ncaption_inputs = {k: v.to(device) for k, v in caption_inputs.items()}\ncaption_outputs = clip_model.text_model(**caption_inputs, output_hidden_states=True)\ntext_emb = caption_outputs.last_hidden_state\ntext_att_mask = caption_inputs[\"attention_mask\"]\n\nnull_inputs = clip_tokenizer([\"\"], padding=True, truncation=True, return_tensors=\"pt\")\nnull_inputs = {k: v.to(device) for k, v in null_inputs.items()}\nnull_outputs = clip_model.text_model(**null_inputs, output_hidden_states=True)\nnull_text_emb = null_outputs.last_hidden_state\nnull_text_att_mask = null_inputs[\"attention_mask\"]\n\nwith torch.no_grad():\n    for t in range(T - 1, -1, -1):\n        timestep = torch.full((batch_size,), t, device=device, dtype=torch.long)\n\n        alpha_bar_t = alpha_bars[t].view(1, 1, 1, 1)\n        alpha_t = alphas[t].view(1, 1, 1, 1)\n        beta_t = betas[t].view(1, 1, 1, 1)\n\n        cond_noise = ddpm(curr_state, text_emb, text_att_mask, timestep)\n        uncond_noise = ddpm(curr_state, null_text_emb, null_text_att_mask, timestep)\n        pred_noise = uncond_noise + cfg_scale * (cond_noise - uncond_noise)\n        \n        coef = beta_t / torch.sqrt(1 - alpha_bar_t)\n        mu_t = (1 / torch.sqrt(alpha_t)) * (curr_state - coef * pred_noise)\n\n        if t > 0:\n            eps = torch.randn_like(curr_state)\n            std_t = torch.sqrt(beta_t)\n            curr_state = mu_t + std_t * eps\n        else:\n            curr_state = mu_t\n\n\nrecon_img = vae.decoder(curr_state)\nrecon_img = recon_img.clamp(0, 1)\nimg = recon_img.squeeze(0).detach().cpu()\nimg = T.ToPILImage()(img)\n\nplt.imshow(img)\nplt.axis(\"off\")\nplt.title(\"Generated Image\")\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}